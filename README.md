# Sampling Assignment - Credit Card Fraud Detection

**Author:** Vani Goyal
**Roll No:** 102303078
**Course:** UCS654

## ğŸ“‹ Table of Contents
- [Introduction](#introduction)
- [Dataset Information](#dataset-information)
- [Methodology](#methodology)
  - [Data Balancing](#data-balancing)
  - [Sampling Techniques](#sampling-techniques)
  - [Machine Learning Models](#machine-learning-models)
- [Results](#results)
- [Analysis & Discussion](#analysis--discussion)
- [Conclusion](#conclusion)
- [How to Run](#how-to-run)
- [Requirements](#requirements)

## ğŸ¯ Introduction

The objective of this assignment is to understand the importance of **sampling techniques** in handling imbalanced datasets and to analyze how different sampling strategies affect the performance of various machine learning models.

In real-world applications, datasets are often highly imbalanced (e.g., fraud detection, disease diagnosis), which can significantly impact model performance. This project explores five different sampling techniques applied to a credit card fraud detection dataset and evaluates their effectiveness across five machine learning models.

## ğŸ“Š Dataset Information

- **Source:** [Credit Card Dataset](https://github.com/AnjulaMehto/Sampling_Assignment/blob/main/Creditcard_data.csv)
- **Original Size:** 772 samples
- **Features:** 30 (Time + V1-V28 + Amount)
- **Target Variable:** Class (0 = Legitimate, 1 = Fraud)
- **Original Class Distribution:**
  - Class 0: 763 samples (98.8%)
  - Class 1: 9 samples (1.2%)
- **Imbalance Ratio:** 84.78:1

### Dataset Characteristics
The dataset contains transactions made by credit cards, where the features V1-V28 are principal components obtained through PCA transformation. The features Time and Amount are not transformed.

## ğŸ”¬ Methodology

### Data Balancing

**Technique Used:** Random Undersampling

To create a balanced dataset as required by the assignment:
1. Identified minority class (Fraud = 9 samples) and majority class (Legitimate = 763 samples)
2. Randomly undersampled the majority class to match the minority class size
3. **Balanced Dataset Size:** 18 samples (9 Fraud + 9 Legitimate)
4. Achieved perfect 1:1 class ratio

**Why Undersampling?**
- Simple and effective for creating balanced base dataset
- Preserves original data characteristics
- Suitable for applying subsequent sampling techniques

### Sampling Techniques

Based on the balanced dataset (18 samples), a sample size of **17 samples** was calculated using the statistical formula:

```
n = (ZÂ² Ã— p Ã— (1-p)) / EÂ²
```

Where:
- Z = 1.96 (95% confidence level)
- p = 0.5 (proportion for balanced dataset)
- E = 0.05 (margin of error)

#### 1. **Sampling1: Simple Random Sampling**
- **Description:** Randomly selects samples without replacement
- **Characteristics:**
  - Each sample has equal probability of selection
  - No systematic bias in selection
- **Implementation:** Used pandas `.sample()` with random_state=42
- **Sample Size:** 17

#### 2. **Sampling2: Stratified Sampling**
- **Description:** Maintains class proportions from the original balanced dataset
- **Characteristics:**
  - Ensures both classes are represented proportionally
  - More representative than simple random sampling
  - Preserves class distribution
- **Implementation:** Manual stratified sampling with proportional allocation
- **Sample Size:** 17 (9 Class 1, 8 Class 0)

#### 3. **Sampling3: Systematic Sampling**
- **Description:** Selects every kth sample at regular intervals
- **Characteristics:**
  - Sampling interval k = population_size / sample_size
  - Random start point between 0 and k-1
  - Ensures spread across dataset
- **Implementation:** Calculated k=1, selected systematically
- **Sample Size:** 17

#### 4. **Sampling4: Cluster Sampling**
- **Description:** Groups data into clusters using K-Means, then selects complete clusters
- **Characteristics:**
  - K-Means clustering applied to features
  - Number of clusters adjusted for small dataset (6 clusters)
  - Randomly selected clusters aggregated
- **Implementation:** K-Means with 6 clusters, selected 6 clusters
- **Sample Size:** 17

#### 5. **Sampling5: Bootstrap Sampling**
- **Description:** Sampling with replacement (samples can repeat)
- **Characteristics:**
  - Allows duplicate samples
  - Foundation of ensemble methods
  - Provides natural variation
- **Implementation:** Random sampling with replacement
- **Sample Size:** 17 (10 unique samples due to replacement)

### Machine Learning Models

Five diverse classification models were selected to represent different learning paradigms:

#### M1: Logistic Regression
- **Type:** Linear classifier
- **Parameters:** max_iter=1000, random_state=42
- **Strengths:** Fast, interpretable, good baseline
- **Best for:** Linearly separable data

#### M2: Decision Tree
- **Type:** Non-linear tree-based classifier
- **Parameters:** max_depth=10, random_state=42
- **Strengths:** Handles non-linear relationships, no feature scaling needed
- **Best for:** Interpretable decision rules

#### M3: Random Forest
- **Type:** Ensemble (bagging) of decision trees
- **Parameters:** n_estimators=100, max_depth=10, random_state=42
- **Strengths:** Reduces overfitting, robust to noise
- **Best for:** High accuracy with feature importance

#### M4: Support Vector Machine (SVM)
- **Type:** Margin-based classifier with RBF kernel
- **Parameters:** kernel='rbf', C=1.0, gamma='scale', random_state=42
- **Preprocessing:** StandardScaler applied
- **Best for:** High-dimensional binary classification

#### M5: Naive Bayes (Gaussian)
- **Type:** Probabilistic classifier
- **Parameters:** Default
- **Strengths:** Fast training, works well with small samples
- **Best for:** Fast predictions, probabilistic outputs

### Evaluation Process

For each of the 25 combinations (5 samplings Ã— 5 models):
1. Split sample into 80% train, 20% test (stratified split)
2. Apply StandardScaler for SVM
3. Train model on training set
4. Predict on test set
5. Calculate accuracy score
6. Store in results matrix

## ğŸ“ˆ Results

### Accuracy Matrix (%)

|                              | Sampling1 | Sampling2 | Sampling3 | Sampling4 | Sampling5 |
|------------------------------|-----------|-----------|-----------|-----------|-----------|
| **M1 (Logistic Regression)** | 75.0      | 25.0      | 75.0      | 75.0      | **100.0** |
| **M2 (Decision Tree)**       | **100.0** | 75.0      | 75.0      | **100.0** | **100.0** |
| **M3 (Random Forest)**       | 25.0      | 50.0      | 75.0      | 25.0      | **100.0** |
| **M4 (SVM)**                 | 50.0      | 25.0      | 75.0      | 50.0      | **100.0** |
| **M5 (Naive Bayes)**         | 25.0      | 0.0       | 75.0      | 25.0      | **100.0** |

### Best Sampling Technique for Each Model

| Model                       | Best Sampling | Accuracy |
|-----------------------------|---------------|----------|
| M1 (Logistic Regression)    | **Sampling5** | 100.0%   |
| M2 (Decision Tree)          | **Sampling1** | 100.0%   |
| M3 (Random Forest)          | **Sampling5** | 100.0%   |
| M4 (SVM)                    | **Sampling5** | 100.0%   |
| M5 (Naive Bayes)            | **Sampling5** | 100.0%   |

### Best Model for Each Sampling Technique

| Sampling Technique | Best Model          | Accuracy |
|--------------------|---------------------|----------|
| Sampling1          | M2 (Decision Tree)  | 100.0%   |
| Sampling2          | M2 (Decision Tree)  | 75.0%    |
| Sampling3          | All Models (Tie)    | 75.0%    |
| Sampling4          | M2 (Decision Tree)  | 100.0%   |
| Sampling5          | All Models (Tie)    | 100.0%   |

### ğŸ† Overall Best Combination
**M1 (Logistic Regression) + Sampling5 (Bootstrap) = 100.0% Accuracy**

*Note: Multiple model-sampling combinations achieved 100% accuracy.*

## ğŸ’¡ Analysis & Discussion

### Key Findings

#### 1. **Sampling5 (Bootstrap) Dominates**

Bootstrap sampling achieved **100% accuracy across all 5 models**, making it the clear winner.

**Why Bootstrap Performed Best:**
- **Sampling with replacement** creates diverse training samples with natural variation
- Duplicates in the sample act as a form of data augmentation
- Reduces variance in model predictions
- Well-suited for small datasets (foundation of bagging methods like Random Forest)
- Creates robust samples that expose models to repeated patterns

**Statistical Insight:** With only 17 samples drawn from 18, bootstrap ensures high overlap while introducing randomness through replacement, creating an optimal training set.

#### 2. **Sampling3 (Systematic) Shows Consistency**

Systematic sampling achieved a **consistent 75% accuracy across all models**, demonstrating:
- Uniform performance across different model types
- Regular interval selection ensures representative coverage
- No bias toward any particular model architecture
- Reliable but not optimal for this small dataset

#### 3. **Sampling2 (Stratified) Underperforms**

Stratified sampling showed the **worst overall performance** (0%-75% range):
- Naive Bayes scored 0% - complete failure on stratified sample
- Average accuracy: 35% (lowest among all techniques)

**Why Stratified Failed:**
- With only 18 balanced samples, strict stratification (9:8 ratio) may have created unfortunate train-test splits
- Small sample size magnifies the impact of individual sample selection
- The attempt to maintain exact proportions may have sacrificed sample diversity

#### 4. **Model-Specific Insights**

**M2 (Decision Tree):**
- **Most robust** - achieved 100% on Sampling1, Sampling4, and Sampling5
- Strong performance (75%+) on all techniques except Sampling2
- Tree structure adapts well to varied sampling approaches

**M1 (Logistic Regression) & M4 (SVM):**
- Both linear/kernel-based models excel with Bootstrap (100%)
- Struggled with Stratified sampling (25%)
- Benefit significantly from sample diversity

**M3 (Random Forest):**
- Surprising variability: 25%-100% range
- Excellent with Bootstrap (100%) - expected since RF uses bootstrapping internally
- Poor with Simple Random (25%) - unexpected for ensemble method

**M5 (Naive Bayes):**
- **Most sensitive** to sampling technique
- Complete failure (0%) on Stratified sampling
- Perfect (100%) on Bootstrap
- Probabilistic nature makes it vulnerable to poor sample distributions

### Comparative Analysis

| Sampling Technique | Avg Accuracy | Std Dev | Best Model Count |
|--------------------|--------------|---------|------------------|
| **Sampling5**      | **100.0%**   | 0.0     | 5                |
| Sampling3          | 75.0%        | 0.0     | 0                |
| Sampling1          | 55.0%        | 30.0    | 0                |
| Sampling4          | 55.0%        | 30.0    | 0                |
| Sampling2          | 35.0%        | 27.4    | 0                |

**Ranking:** Sampling5 > Sampling3 > (Sampling1 = Sampling4) > Sampling2

### Why Small Sample Size Matters

The balanced dataset had only **18 samples**, leading to:
1. **High variance in results** - single samples significantly impact accuracy
2. **Test set size = 3-4 samples** - accuracy jumps in 25% increments
3. **Amplified sampling effects** - technique choice critically important
4. **Bootstrap advantage** - replacement creates more training diversity

### Practical Implications

**For Practitioners:**
1. **Bootstrap sampling** should be the default choice for small, balanced datasets
2. **Avoid stratified sampling** when sample size is very small (<50 samples)
3. **Systematic sampling** provides consistent, safe middle-ground performance
4. **Decision Trees** are most robust to sampling technique variations
5. **Naive Bayes** requires careful sampling - highly sensitive to sample quality

**For Imbalanced Datasets:**
- Balance first (undersampling/oversampling/SMOTE)
- Then apply bootstrap sampling for robust model training
- Ensemble methods (Random Forest) naturally benefit from bootstrap approach

## ğŸ“ Conclusion

This assignment successfully demonstrated the critical impact of sampling techniques on machine learning model performance, especially with small, balanced datasets.

**Key Takeaways:**
1. âœ… **Bootstrap sampling (Sampling5) is the clear winner**, achieving 100% accuracy across all five models
2. âœ… **Systematic sampling (Sampling3) provides reliable consistency** at 75% across all models
3. âŒ **Stratified sampling (Sampling2) failed** on this small dataset, showing that technique suitability depends on sample size
4. ğŸ¯ **Decision Tree (M2) proved most robust** to sampling variations
5. âš ï¸ **Naive Bayes (M5) most sensitive** to sampling quality
6. ğŸ“Š **Sampling technique choice is crucial** for small datasets - can mean difference between 0% and 100% accuracy

**Final Recommendation:**
For credit card fraud detection with small, balanced datasets, use **Bootstrap Sampling + Logistic Regression** or **Bootstrap Sampling + Decision Tree** for optimal performance (100% accuracy).

The assignment highlights that in machine learning, **how you sample your data can be just as important as which model you choose**, particularly when working with limited data.

## ğŸš€ How to Run

### Prerequisites
- Python 3.8+
- Virtual environment (recommended)

### Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd Sampling-Vani-102303078
```

2. Activate virtual environment:
```bash
source .venv/bin/activate  # On macOS/Linux
# OR
.venv\Scripts\activate  # On Windows
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Execution

Run the main script:
```bash
python main.py
```

### Output Files
- `Creditcard_data.csv` - Downloaded dataset
- `results.csv` - Accuracy matrix in CSV format
- Terminal output with detailed analysis

## ğŸ“¦ Requirements

```
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0
requests>=2.28.0
```

## ğŸ“ Repository Structure

```
Sampling-Vani-102303078/
â”œâ”€â”€ main.py                    # Main implementation
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ Sampling_Assignment.pdf    # Assignment instructions
â”œâ”€â”€ Creditcard_data.csv        # Downloaded dataset
â”œâ”€â”€ results.csv                # Generated results
â””â”€â”€ .venv/                     # Virtual environment
```

## ğŸ”— References

- Dataset Source: [AnjulaMehto/Sampling_Assignment](https://github.com/AnjulaMehto/Sampling_Assignment)
- Scikit-learn Documentation: [https://scikit-learn.org/](https://scikit-learn.org/)
- Sampling Theory: Statistical Methods for Data Science

---

**Assignment Submitted By:**
**Vani Goyal**
**Roll No: 102303078**
**UCS654 - Predictive Analytics Using Statistics**
